# åœ¨ OpenShift ä¸Šéƒ¨ç½²LLMï¼Œä½¿ç”¨Red Hat Inference Server å®Œæ•´éƒ¨ç½²æŒ‡å—

## ç¯å¢ƒç¡®è®¤ âœ…
ç¡®ä¿ä»¥ä¸‹ç»„ä»¶å·²å°±ç»ªï¼š
- KServe æ§åˆ¶å™¨è¿è¡Œæ­£å¸¸
- Knative Serving æ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸
- Istio ç³»ç»Ÿç»„ä»¶è¿è¡Œæ­£å¸¸
- DataScienceCluster çŠ¶æ€ä¸º Ready

**å¿…éœ€çš„Operatorså·²å®‰è£…**ï¼š
- âœ… **NVIDIA GPU Operator** - æä¾›GPUæ”¯æŒ
- âœ… **Red Hat OpenShift AI** - æä¾›AI/MLå¹³å°åŠŸèƒ½
- âœ… **Red Hat OpenShift Serverless** - æä¾›Knative Servingæ”¯æŒ
- âœ… **Red Hat OpenShift Service Mesh 2** - æä¾›IstioæœåŠ¡ç½‘æ ¼æ”¯æŒ
- âœ… **Node Feature Discovery Operator** - è‡ªåŠ¨å‘ç°èŠ‚ç‚¹ç‰¹æ€§
- âœ… **Package Server** - ç®¡ç†OperatoråŒ…

**éªŒè¯OperatorsçŠ¶æ€**ï¼š
```bash
# æ£€æŸ¥å¿…éœ€çš„OperatorsçŠ¶æ€
oc get csv -A | grep -E "(gpu-operator|rhods|serverless|servicemesh|nfd)"

# æŸ¥çœ‹DataScienceClusterçŠ¶æ€
oc get datasciencecluster -A
```

---

## ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºå¹¶åˆ‡æ¢åˆ°å·¥ä½œå‘½åç©ºé—´

```bash
# åˆ›å»ºä¸“ç”¨å‘½åç©ºé—´
oc new-project ai-inference-demo

# ç¡®è®¤å½“å‰é¡¹ç›®
oc project ai-inference-demo
```

---

## ç¬¬äºŒæ­¥ï¼šé…ç½®å‘½åç©ºé—´ä¸ºService Meshæˆå‘˜

```bash
# ç»™å‘½åç©ºé—´æ·»åŠ Istio injectionæ ‡ç­¾
oc label namespace ai-inference-demo istio-injection=enabled

# æ£€æŸ¥æ˜¯å¦æœ‰ServiceMeshMemberRolléœ€è¦æ›´æ–°
oc get servicemeshmemberroll -A

# å¦‚æœå­˜åœ¨ServiceMeshMemberRollï¼Œå°†å‘½åç©ºé—´æ·»åŠ åˆ°æˆå‘˜åˆ—è¡¨
oc patch servicemeshmemberroll default -n istio-system --type='json' -p='[{"op": "add", "path": "/spec/members/-", "value": "ai-inference-demo"}]'

# éªŒè¯å‘½åç©ºé—´æ ‡ç­¾
oc get namespace ai-inference-demo --show-labels

# å¯ç”¨ anyuid SCCï¼Œé¿å… token å’Œæƒé™é—®é¢˜
oc adm policy add-scc-to-user anyuid -z default -n ai-inference-demo
```

---

## ç¬¬ä¸‰æ­¥ï¼šé…ç½®Red Hat Registryé•œåƒæ‹‰å–æƒé™

```bash
# åˆ›å»ºRed Hat Registryçš„pull secretï¼ˆéœ€è¦æœ‰æ•ˆçš„Red Hat Customer Portalå‡­è¯ï¼‰
oc create secret docker-registry redhat-registry-secret \
    --docker-server=registry.redhat.io \
    --docker-username=YOUR_RH_USERNAME \
    --docker-password='YOUR_RH_PASSWORD' \
    --docker-email=YOUR_EMAIL

# å°†secreté“¾æ¥åˆ°é»˜è®¤service account
oc secrets link default redhat-registry-secret --for=pull
oc secrets link deployer redhat-registry-secret --for=pull

# éªŒè¯secretåˆ›å»º
oc get secret redhat-registry-secret
```

**æ³¨æ„**: è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…Red Hat Customer Portalå‡­è¯

---

## ç¬¬å››æ­¥ï¼šåˆ›å»ºServingRuntime

**é•œåƒç‰ˆæœ¬è¯´æ˜**: `registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1752784628` æ˜¯ç›®å‰æœ€æ–°ç‰ˆæœ¬

ğŸ’¡ **æç¤º**: æœ€æ–°ç‰ˆæœ¬å¯åœ¨ [Red Hat Catalog](https://catalog.redhat.com/en) æœç´¢å…³é”®å­— `rhaiis` è·å–

```bash
cat <<EOF | oc apply -f -
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: red-hat-vllm-runtime
  namespace: ai-inference-demo
spec:
  supportedModelFormats:
    - name: vllm
      version: "1"
      autoSelect: true
    - name: pytorch
      version: "1"
      autoSelect: true
  containers:
    - name: kserve-container
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.0-1752784628
      ports:
        - containerPort: 8080
          name: http1
          protocol: TCP
      command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
      args:
        - "--model"
        - "/mnt/models/DialoGPT-small"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8080"
        - "--served-model-name"
        - "DialoGPT-small"
        - "--max-model-len"
        - "1024"
        - "--disable-log-requests"
      env:
        - name: VLLM_CPU_KVCACHE_SPACE
          value: "4"
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: TRANSFORMERS_OFFLINE
          value: "1"
      resources:
        requests:
          cpu: "1"
          memory: "4Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "2"
          memory: "8Gi"
          nvidia.com/gpu: "1"
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 120
        periodSeconds: 10
        timeoutSeconds: 10
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        initialDelaySeconds: 180
        periodSeconds: 30
        timeoutSeconds: 10
EOF
```

---

## ç¬¬äº”æ­¥ï¼šéªŒè¯ServingRuntime

```bash
# æ£€æŸ¥ServingRuntimeçŠ¶æ€
oc get servingruntime red-hat-vllm-runtime

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
oc describe servingruntime red-hat-vllm-runtime
```

---

## ç¬¬å…­æ­¥ï¼šåˆ›å»ºPVCç”¨äºæ¨¡å‹å­˜å‚¨

```bash
cat <<EOF | oc apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-storage-pvc
  namespace: ai-inference-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp3-csi
EOF

# éªŒè¯PVCåˆ›å»º
oc get pvc model-storage-pvc
```

---

## ç¬¬ä¸ƒæ­¥ï¼šä¸‹è½½æ¨¡å‹åˆ°PVC

```bash
cat <<EOF | oc apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: dialogpt-model-downloader
  namespace: ai-inference-demo
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: downloader
        image: python:3.12-slim
        command:
        - /bin/sh
        - -c
        - |
          set -e
          export HOME=/tmp
          pip install --no-cache-dir --user huggingface_hub
          export PATH="\$HOME/.local/bin:\$PATH"
          mkdir -p /models/DialoGPT-small
          python3 -c "from huggingface_hub import hf_hub_download; files = ['config.json', 'pytorch_model.bin', 'tokenizer_config.json', 'vocab.json', 'merges.txt']; [hf_hub_download(repo_id='microsoft/DialoGPT-small', filename=f, local_dir='/models/DialoGPT-small') for f in files]"
          rm /models/DialoGPT-small/tokenizer.json || true
          ls -la /models/DialoGPT-small/
          du -sh /models/DialoGPT-small/pytorch_model.bin
        volumeMounts:
        - name: model-storage
          mountPath: /models
        env:
        - name: HF_TOKEN
          value: "YOUR_HF_TOKEN_HERE"
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage-pvc
EOF
```

**æ³¨æ„**: å¦‚æœéœ€è¦è®¿é—®ç§æœ‰æ¨¡å‹ï¼Œè¯·æ›¿æ¢ `YOUR_HF_TOKEN_HERE` ä¸ºæ‚¨çš„Hugging Face Token

---

## ç¬¬å…«æ­¥ï¼šç›‘æ§æ¨¡å‹ä¸‹è½½è¿›åº¦

```bash
# æŸ¥çœ‹JobçŠ¶æ€
oc get jobs

# æŸ¥çœ‹ä¸‹è½½æ—¥å¿—
oc logs job/dialogpt-model-downloader -f

# ç­‰å¾…çœ‹åˆ° "Download completed!" æ¶ˆæ¯
```

---

## ç¬¬ä¹æ­¥ï¼šéªŒè¯æ¨¡å‹æ–‡ä»¶ä½ç½®

```bash
# åˆ›å»ºè°ƒè¯•Podæ£€æŸ¥PVCä¸­çš„æ¨¡å‹æ–‡ä»¶ä½ç½®
cat <<EOF | oc apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: pvc-explorer
  namespace: ai-inference-demo
spec:
  restartPolicy: Never
  containers:
  - name: explorer
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["sleep", "300"]
    volumeMounts:
    - name: model-storage
      mountPath: /data
  volumes:
  - name: model-storage
    persistentVolumeClaim:
      claimName: model-storage-pvc
EOF

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ä½ç½® - ç¡®å®šPVCä¸Šå·²ç»æœ‰ä¸‹è½½çš„LLM
oc exec pvc-explorer -- ls -la /data/
oc exec pvc-explorer -- ls -la /data/DialoGPT-small/
oc exec pvc-explorer -- find /data -name "config.json"
oc exec pvc-explorer -- du -h /data/DialoGPT-small/pytorch_model.bin

# éªŒè¯å†…å®¹ï¼ˆå¿…é¡»ï¼‰
oc exec pvc-explorer -- head -n 5 /data/DialoGPT-small/config.json
oc exec pvc-explorer -- head -n 5 /data/DialoGPT-small/tokenizer_config.json

# æ¸…ç†è°ƒè¯•Pod
oc delete pod pvc-explorer
```

**é¢„æœŸç»“æœ**ï¼šåº”è¯¥çœ‹åˆ° `/data/DialoGPT-small/` ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š
- `config.json`
- `pytorch_model.bin`
- `tokenizer_config.json`
- `vocab.json`
- `merges.txt`

---

## ç¬¬åæ­¥ï¼šåˆ›å»ºInferenceService

```bash
cat <<EOF | oc apply -f -
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: dialogpt-small-service
  namespace: ai-inference-demo
  annotations:
    sidecar.istio.io/inject: "false"  # ç¦ç”¨ Istio sidecarï¼Œé¿å… envoy æŠ¥é”™
    serving.kserve.io/enable-service-account-token-mount: "true"  # æŒ‚è½½ tokenï¼Œè§£å†³è®¤è¯å¤±è´¥
spec:
  predictor:
    model:
      modelFormat:
        name: pytorch
      runtime: red-hat-vllm-runtime
      storageUri: pvc://model-storage-pvc
      resources:
        requests:
          cpu: "1"
          memory: "4Gi"
          nvidia.com/gpu: "1"
        limits:
          cpu: "2"
          memory: "8Gi"
          nvidia.com/gpu: "1"
      env:
        - name: VLLM_GPU_MEMORY_UTILIZATION
          value: "0.5"
EOF
```

**æ¨¡å‹ä¿¡æ¯**ï¼š
- **microsoft/DialoGPT-small**: 117MBï¼Œ117Må‚æ•°
- **æœ¬åœ°å­˜å‚¨**: ä»PVCåŠ è½½ï¼Œå¯åŠ¨å¿«é€Ÿä¸”ç¨³å®š
- **å¯¹è¯ç”Ÿæˆ**: é€‚åˆæµ‹è¯•æ¨ç†åŠŸèƒ½
- **vLLMä¼˜åŒ–**: ä½¿ç”¨vLLMæ¨ç†å¼•æ“ï¼Œæ€§èƒ½æ›´å¥½

---

## ç¬¬åä¸€æ­¥ï¼šç›‘æ§éƒ¨ç½²çŠ¶æ€

```bash
# å®æ—¶ç›‘æ§InferenceServiceçŠ¶æ€
oc get inferenceservice dialogpt-small-service -w

# æŸ¥çœ‹ç›¸å…³pods
oc get pods -l serving.kserve.io/inferenceservice=dialogpt-small-service

# æŸ¥çœ‹è¯¦ç»†çŠ¶æ€
oc describe inferenceservice dialogpt-small-service

# æŸ¥çœ‹äº‹ä»¶
oc get events --sort-by='.lastTimestamp' | head -20
```

**æˆåŠŸæ ‡å¿—**ï¼šå½“çœ‹åˆ° `READY=True` æ—¶ï¼Œè¡¨ç¤ºæœåŠ¡å·²æˆåŠŸå¯åŠ¨ã€‚

---

## ç¬¬åäºŒæ­¥ï¼šç®€å•å¯¹è¯æµ‹è¯•

**é‡è¦è¯´æ˜**: DialoGPT-smallæ˜¯ä¸€ä¸ªå°å‹å¯¹è¯æ¨¡å‹ï¼ˆ117Må‚æ•°ï¼‰ï¼Œå›å¤è´¨é‡æœ‰é™ï¼Œæœ‰æ—¶å¯èƒ½ç”Ÿæˆä¸å¤Ÿè¿è´¯çš„å†…å®¹ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚

### æœ€ç®€å•çš„æµ‹è¯•æ–¹æ³•

```bash
# è®¾ç½®å˜é‡
PREDICTOR_POD=$(oc get pods -l serving.kserve.io/inferenceservice=dialogpt-small-service -o jsonpath='{.items[0].metadata.name}')

# åŸºç¡€å¥åº·æ£€æŸ¥
echo "=== å¥åº·æ£€æŸ¥ ==="
oc exec $PREDICTOR_POD -c kserve-container -- curl -s localhost:8080/health

# å¯¹è¯æµ‹è¯•1ï¼šç®€å•é—®å€™
echo -e "\n=== æˆ‘é—®ï¼šHello, how are you? ==="
oc exec $PREDICTOR_POD -c kserve-container -- curl -s -X POST localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "DialoGPT-small",
    "messages": [{"role": "user", "content": "Hello, how are you?"}],
    "max_tokens": 30,
    "temperature": 0.7
  }'

# å¯¹è¯æµ‹è¯•2ï¼šè¯¢é—®åå­—
echo -e "\n=== æˆ‘é—®ï¼šWhat is your name? ==="
oc exec $PREDICTOR_POD -c kserve-container -- curl -s -X POST localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "DialoGPT-small",
    "messages": [{"role": "user", "content": "What is your name?"}],
    "max_tokens": 20,
    "temperature": 0.8
  }'

# å¯¹è¯æµ‹è¯•3ï¼šç®€å•é—®é¢˜
echo -e "\n=== æˆ‘é—®ï¼šHi ==="
oc exec $PREDICTOR_POD -c kserve-container -- curl -s -X POST localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "DialoGPT-small",
    "messages": [{"role": "user", "content": "Hi"}],
    "max_tokens": 10,
    "temperature": 0.5
  }'
```

---

## ç¬¬åä¸‰æ­¥ï¼šæ€§èƒ½å’Œç›‘æ§æ£€æŸ¥

### æŸ¥çœ‹èµ„æºä½¿ç”¨æƒ…å†µ

```bash
# æŸ¥çœ‹Podèµ„æºä½¿ç”¨ï¼ˆéœ€è¦metrics-serveræ”¯æŒï¼‰
oc adm top pod -l serving.kserve.io/inferenceservice=dialogpt-small-service

# å¦‚æœä¸Šé¢å‘½ä»¤ä¸å·¥ä½œï¼Œä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ³•ï¼š

# æŸ¥çœ‹Podçš„èµ„æºé…ç½®å’Œé™åˆ¶
PREDICTOR_POD=$(oc get pods -l serving.kserve.io/inferenceservice=dialogpt-small-service -o jsonpath='{.items[0].metadata.name}')
oc describe pod $PREDICTOR_POD | grep -A10 -B5 "Limits\|Requests"

# æŸ¥çœ‹PodçŠ¶æ€å’Œè¿è¡Œæ—¶é—´
oc get pod $PREDICTOR_POD -o wide

# æŸ¥çœ‹èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
oc adm top nodes

# å¦‚æœmetrics-serverä¸å¯ç”¨ï¼ŒæŸ¥çœ‹PodåŸºæœ¬ä¿¡æ¯
oc get pod $PREDICTOR_POD -o jsonpath='{.status.containerStatuses[*].restartCount}'
echo " (é‡å¯æ¬¡æ•°)"
```

### æœåŠ¡çŠ¶æ€æ£€æŸ¥

```bash
# æ£€æŸ¥InferenceServiceæ•´ä½“çŠ¶æ€
oc get inferenceservice dialogpt-small-service -o yaml | grep -A20 status

# æŸ¥çœ‹æ‰€æœ‰ç›¸å…³èµ„æºçŠ¶æ€
oc get pods,svc,inferenceservice -l serving.kserve.io/inferenceservice=dialogpt-small-service

# æŸ¥çœ‹æœ€è¿‘çš„é›†ç¾¤äº‹ä»¶
oc get events --sort-by='.lastTimestamp' | head -20

# æ£€æŸ¥æœåŠ¡ç«¯ç‚¹
oc get endpoints dialogpt-small-service-predictor
```

---

## å¸¸è§é—®é¢˜æ’æŸ¥

### 1. æ¨ç†æœåŠ¡æ— æ³•è®¿é—®

```bash
# æ£€æŸ¥serviceçŠ¶æ€
oc get svc | grep dialogpt-small-service

# æ£€æŸ¥endpoints
oc get endpoints dialogpt-small-service-predictor

# æ£€æŸ¥podsçŠ¶æ€
oc get pods -l serving.kserve.io/inferenceservice=dialogpt-small-service
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æŸ¥çœ‹podäº‹ä»¶
oc describe pod $PREDICTOR_POD

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
oc exec $PREDICTOR_POD -c kserve-container -- ls -la /mnt/models/DialoGPT-small/

# æŸ¥çœ‹vLLMå¯åŠ¨æ—¥å¿—
oc logs $PREDICTOR_POD -c kserve-container | grep -i error
```

### 3. å†…å­˜æˆ–GPUèµ„æºä¸è¶³

```bash
# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
oc describe nodes | grep -A5 -B5 "Allocated resources"

# é™ä½èµ„æºè¦æ±‚
oc patch inferenceservice dialogpt-small-service --type='merge' -p='{
  "spec": {
    "predictor": {
      "model": {
        "resources": {
          "requests": {"cpu": "500m", "memory": "2Gi"},
          "limits": {"cpu": "1", "memory": "4Gi"}
        }
      }
    }
  }
}'
```

---

## æ¸…ç†èµ„æº

```bash
# åˆ é™¤æµ‹è¯•Pod
oc delete pod inference-test-client

# åˆ é™¤InferenceService
oc delete inferenceservice dialogpt-small-service

# åˆ é™¤ServingRuntime
oc delete servingruntime red-hat-vllm-runtime

# åˆ é™¤ä¸‹è½½Job
oc delete job dialogpt-model-downloader

# åˆ é™¤PVCï¼ˆæ³¨æ„ï¼šè¿™ä¼šåˆ é™¤æ‰€æœ‰ä¸‹è½½çš„æ¨¡å‹ï¼‰
oc delete pvc model-storage-pvc

# åˆ é™¤Pull Secret
oc delete secret redhat-registry-secret

# åˆ é™¤æ•´ä¸ªé¡¹ç›®
oc delete project ai-inference-demo
```

---

## æ€»ç»“

è¿™ä¸ªæŒ‡å—æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„Red Hat Inference Serveréƒ¨ç½²å’Œå†…éƒ¨æµ‹è¯•æµç¨‹ï¼š

âœ… **ä¼˜åŠ¿**ï¼š
- ğŸ”’ **å®‰å…¨**: æ‰€æœ‰æµ‹è¯•éƒ½åœ¨é›†ç¾¤å†…éƒ¨è¿›è¡Œï¼Œæ— éœ€æš´éœ²å¤–éƒ¨ç«¯ç‚¹
- âš¡ **é«˜æ•ˆ**: ä½¿ç”¨PVCæœ¬åœ°å­˜å‚¨ï¼Œæ¨¡å‹åŠ è½½å¿«é€Ÿ
- ğŸ› ï¸ **çµæ´»**: æ”¯æŒå¤šç§æµ‹è¯•æ–¹æ³•å’Œäº¤äº’æ–¹å¼
- ğŸ“Š **å¯è§‚æµ‹**: æä¾›è¯¦ç»†çš„ç›‘æ§å’Œæ—¥å¿—æŸ¥çœ‹æ–¹æ³•

âœ… **é€‚ç”¨åœºæ™¯**ï¼š
- å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒéªŒè¯
- å†…éƒ¨APIé›†æˆæµ‹è¯•
- æ¨¡å‹æ€§èƒ½è¯„ä¼°
- å®‰å…¨åˆè§„ç¯å¢ƒä¸‹çš„AIæœåŠ¡éƒ¨ç½²

æŒ‰ç…§è¿™ä¸ªæŒ‡å—ï¼Œæ‚¨å¯ä»¥åœ¨ä¸åˆ›å»ºå¤–éƒ¨è·¯ç”±çš„æƒ…å†µä¸‹ï¼Œå®Œæ•´åœ°éƒ¨ç½²å’Œæµ‹è¯•Red Hat Inference Serverï¼
